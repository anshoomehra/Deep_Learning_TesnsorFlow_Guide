{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created By: Anshoo Mehra\n",
    "Dated: September 21st 2017\n",
    "\n",
    "# Tensorflow Guide Chronological Order ..\n",
    "\n",
    "All Credit to Udacity, all labs & explanation below is excerpt from class labs. Intent is to try all key examples & keep refrence to key highlights for future reference.\n",
    "\n",
    "I created this notebook for personal reference, however if this helps anyone in general please feel free to refer & consider Udacity policy/lceinses prior sharing ahead. I apologize documentation is fairly limited, it is just to the point to revise concepts than using this as primary guide to learn .. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hello World!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello World!'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create TensorFlow object called hello_constant\n",
    "hello_constant = tf.constant('Hello World!')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Run the tf.constant operation in the session\n",
    "    output = sess.run(hello_constant)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholders (Can't be modified like as Constant shown above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def run():\n",
    "    output = None\n",
    "    x = tf.placeholder(tf.int32)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        # TODO: Feed the x tensor 123\n",
    "        output = sess.run(x, feed_dict={x:123})\n",
    "\n",
    "    return output\n",
    "\n",
    "print(run())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "123\n",
      "124\n"
     ]
    }
   ],
   "source": [
    "vx = tf.Variable(5)\n",
    "\n",
    "# The tf.Variable class creates a tensor with an initial value that\n",
    "# can be modified, much like a normal Python variable. This tensor\n",
    "# stores its state in the session, so you must initialize the state\n",
    "# of the tensor manually. You'll use the tf.global_variables_initializer()\n",
    "# function to initialize the state of all the Variable tensors.\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(sess.run(vx))\n",
    "    print(sess.run(vx.assign(123)))\n",
    "    print(sess.run(vx.assign_add(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Math Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 6 10 2\n"
     ]
    }
   ],
   "source": [
    "x = tf.add(5, 2) #7\n",
    "y = tf.subtract(10, 4) # 6\n",
    "z = tf.multiply(2, 5)  # 10\n",
    "f = tf.div(4,2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "        x_ = sess.run(x)\n",
    "        y_ = sess.run(y)\n",
    "        z_ = sess.run(z)\n",
    "        f_ = sess.run(f)\n",
    "\n",
    "print (x_, y_, z_, f_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with tf.Session() as sess:\n",
    "    #sess.run(tf.subtract(tf.constant(2.0),tf.constant(1))) \n",
    "\n",
    "## TRY TO UNCOMMENT ABOVE TO SEE THE ERROR MESSAGE..\n",
    "\n",
    "# Fails with ValueError: Tensor conversion requested dtype float32 for Tensor with dtype int32:\n",
    "# That's because the constant 1 is an integer but the constant 2.0 \n",
    "# is a floating point value and subtract expects them to match.# In cases like these, you can either make sure your data is all \n",
    "# of the same type, or you can cast a value to another type. In this case,\n",
    "# converting the 2.0 to an integer before subtracting, like so, will give \n",
    "# the correct result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# In cases like these, you can either make sure your data is all \n",
    "# of the same type, or you can cast a value to another type. In this case,\n",
    "# converting the 2.0 to an integer before subtracting, like so, will give \n",
    "# the correct result:\n",
    "\n",
    "dummy = tf.subtract(tf.cast(tf.constant(2.0), tf.int32), tf.constant(1))   # 1\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print (sess.run(dummy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Math Function & Type Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# TODO: Convert the following to TensorFlow:\n",
    "#x = 10\n",
    "#y = 2\n",
    "#z = x/y - 1\n",
    "\n",
    "# TODO: Print z from a session\n",
    "x = tf.constant(10)\n",
    "y = tf.constant(2)\n",
    "z = tf.subtract(tf.div(x,y),tf.constant(1))\n",
    "f = tf.cast(z, tf.float32)\n",
    "## OR z = tf.div(x,y) - 1\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(z)\n",
    "    print(output)\n",
    "    output = sess.run(f)\n",
    "    print(output)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assuming everyone knows Linear Combination Equation y = xW + b \n",
    "\n",
    "y = Logits\n",
    "x = Inputs\n",
    "W = Weights\n",
    "b = Bias\n",
    "\n",
    "Linear Equation will give is Logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Softmax\n",
    "\n",
    "Softmax Function Convert Logits to Probabilities (All Probabilities Add to 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.8360188   0.11314284  0.05083836]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    # TODO: Compute and return softmax(x)\n",
    "    return np.exp(x)/np.sum(np.exp(x), axis=0)\n",
    "        \n",
    "logits = [3.0, 1.0, 0.2]\n",
    "print(softmax(logits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax in Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.65900117  0.24243298  0.09856589]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def run():\n",
    "    output = None\n",
    "    logit_data = [2.0, 1.0, 0.1]\n",
    "    logits = tf.placeholder(tf.float32)\n",
    "\n",
    "    softmax = tf.nn.softmax(logits)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        output = sess.run(softmax, feed_dict={logits: logit_data})\n",
    "\n",
    "    return output\n",
    "\n",
    "print(run())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding \n",
    "\n",
    "y = [ .5, .6, .9]\n",
    "\n",
    "y_one_hot_encoded = [ 0, 0, 1]\n",
    "\n",
    "So the hightest probability gets marked 1 and rest 0 as simple as that ...\n",
    "\n",
    "One Hot Encoding works really well but One problem with One Hot Encoding is that once classes grow to thousands or millions, vector becomes really large & very inefficient. This can be solved with embeddings, we will see that later.\n",
    "\n",
    "However, this method makes the process very simple, we have just 2 vectors, one vector contains probabilities of classes and other vector one-hot encodeding for labels. So if we have a way to measure distance between these two Vectors, that can help us classify and this process of measuring distance is called **Cross-Entropy.\n",
    "\n",
    "**Cross-Entropy will give us small distance for Right Class and higher distance for Incorrect Classes.**\n",
    "\n",
    "### Cross Entropy\n",
    "\n",
    "D(S,L) = - Σᵢ Lᵢ * log(Sᵢ)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Process is called \" Multinomial Logistics Classification \"\n",
    "\n",
    "\n",
    "INPUT **--linear model--**> LOGITS > **--softmax--** > PROBABILITIES > **--cross_entropy--** < 1_HOT_LABELS\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's see how to compute Cross Entropy in Tesnforflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.356675\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "softmax_data = [0.7, 0.2, 0.1]\n",
    "one_hot_data = [1.0, 0.0, 0.0]\n",
    "\n",
    "softmax = tf.placeholder(tf.float32)\n",
    "one_hot = tf.placeholder(tf.float32)\n",
    "\n",
    "# ToDo: Print cross entropy from session\n",
    "cross_entropy = -tf.reduce_sum(tf.multiply(one_hot, tf.log(softmax)))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(cross_entropy, feed_dict={softmax: softmax_data, one_hot: one_hot_data}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Almost there, since most of us will be trying to train models on CPU, concept called mini-batching is something we must learn to not run short on memory and be stranded in middle of nowhere ..\n",
    "\n",
    "Mini-batching is a technique for training on subsets of the dataset instead of all the data at one time. This provides the ability to train a model, even if a computer lacks the memory to store the entire dataset.\n",
    "\n",
    "Mini-batching is computationally inefficient, since you can't calculate the loss simultaneously across all samples. However, this is a small price to pay in order to be able to run the model at all.\n",
    "\n",
    "It's also quite useful combined with SGD. The idea is to randomly shuffle the data at the start of each epoch, then create the mini-batches. For each mini-batch, you train the network weights with gradient descent. Since these batches are random, you're performing SGD with each batch.\n",
    "\n",
    "**TensorFlow Mini-batching**\n",
    "In order to use mini-batching, you must first divide your data into batches.\n",
    "\n",
    "Unfortunately, it's sometimes impossible to divide the data into batches of exactly equal size. For example, imagine you'd like to create batches of 128 samples each from a dataset of 1000 samples. Since 128 does not evenly divide into 1000, you'd wind up with 7 batches of 128 samples, and 1 batch of 104 samples. (7*128 + 1*104 = 1000)\n",
    "\n",
    "In that case, the size of the batches would vary, so you need to take advantage of TensorFlow's tf.placeholder() function to receive the varying batch sizes.\n",
    "\n",
    "Continuing the example, if each sample had n_input = 784 features and n_classes = 10 possible labels, the dimensions for features would be [None, n_input] and labels would be [None, n_classes].\n",
    "\n",
    "features = tf.placeholder(tf.float32, [**None**, n_input]) <br>\n",
    "labels = tf.placeholder(tf.float32, [**None**, n_classes])\n",
    "\n",
    "What does None do here?\n",
    "\n",
    "The None dimension is a placeholder for the batch size. At runtime, TensorFlow will accept any batch size greater than 0.\n",
    "\n",
    "Going back to our earlier example, this setup allows you to feed features and labels into the model as either the batches of 128 samples or the single batch of 104 samples.\n",
    "\n",
    "Let's understand it better with a example .. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[['F11', 'F12', 'F13', 'F14'],\n",
      "   ['F21', 'F22', 'F23', 'F24'],\n",
      "   ['F31', 'F32', 'F33', 'F34']],\n",
      "  [['L11', 'L12'], ['L21', 'L22'], ['L31', 'L32']]],\n",
      " [[['F41', 'F42', 'F43', 'F44']], [['L41', 'L42']]]]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "def batches(batch_size, features, labels):\n",
    "    \"\"\"\n",
    "    Create batches of features and labels\n",
    "    :param batch_size: The batch size\n",
    "    :param features: List of features\n",
    "    :param labels: List of labels\n",
    "    :return: Batches of (Features, Labels)\n",
    "    \"\"\"\n",
    "    assert len(features) == len(labels)\n",
    "    # TODO: Implement batching\n",
    "    outout_batches = []\n",
    "    \n",
    "    sample_size = len(features)\n",
    "    for start_i in range(0, sample_size, batch_size):\n",
    "        end_i = start_i + batch_size\n",
    "        batch = [features[start_i:end_i], labels[start_i:end_i]]\n",
    "        outout_batches.append(batch)\n",
    "        \n",
    "    return outout_batches\n",
    "\n",
    "\n",
    "# 4 Samples of features\n",
    "example_features = [\n",
    "    ['F11','F12','F13','F14'],\n",
    "    ['F21','F22','F23','F24'],\n",
    "    ['F31','F32','F33','F34'],\n",
    "    ['F41','F42','F43','F44']]\n",
    "\n",
    "# 4 Samples of labels\n",
    "example_labels = [\n",
    "    ['L11','L12'],\n",
    "    ['L21','L22'],\n",
    "    ['L31','L32'],\n",
    "    ['L41','L42']]\n",
    "\n",
    "example_batches = batches(3, example_features, example_labels)\n",
    "\n",
    "pprint(example_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epochs \n",
    "\n",
    "With mini-batching the accuracy is low, but you probably know that you could train on the dataset more than once. You can train a model using the dataset multiple times.\n",
    "\n",
    "An **epoch** is a single forward and backward pass of the whole dataset. This is used to increase the accuracy of the model without requiring more data. This section will cover epochs in TensorFlow and how to choose the right number of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So, let's put it all together and run the below lab. \n",
    "\n",
    "**If it appear confusing, try to going to last commneted lab & understand one step at a time & if need be going back to above examples .. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Handwritten Numbers (Image) Classification using MNIST database by applying ..\n",
    "### Linear Function / Logistics Regression ( y= xW + b )with Tensforflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the weights with random numbers from a normal distribution is good practice. Randomizing the weights helps the model from becoming stuck in the same place every time you train it.\n",
    "\n",
    "Similarly, choosing weights from a normal distribution prevents any one weight from overwhelming other weights. You'll use the tf.truncated_normal() function to generate random numbers from a normal distribution.\n",
    "\n",
    "The tf.truncated_normal() function returns a tensor with random values from a normal distribution whose magnitude is no more than 2 standard deviations from the mean.\n",
    "\n",
    "Since the weights are already helping prevent the model from getting stuck, you don't need to randomize the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def weights(n_features, n_labels):\n",
    "    \"\"\"\n",
    "    Return TensorFlow weights\n",
    "    :param n_features: Number of features\n",
    "    :param n_labels: Number of labels\n",
    "    :return: TensorFlow weights\n",
    "    \"\"\"\n",
    "    # TODO: Return weights\n",
    "    return tf.Variable(tf.random_normal([n_features, n_labels]))\n",
    "\n",
    "\n",
    "def biases(n_labels):\n",
    "    \"\"\"\n",
    "    Return TensorFlow bias\n",
    "    :param n_labels: Number of labels\n",
    "    :return: TensorFlow bias\n",
    "    \"\"\"\n",
    "    # TODO: Return biases\n",
    "    return tf.Variable(tf.random_normal[n_labels])\n",
    "\n",
    "\n",
    "def linear(input, w, b):\n",
    "    \"\"\"\n",
    "    Return linear function in TensorFlow\n",
    "    :param input: TensorFlow input\n",
    "    :param w: TensorFlow weights\n",
    "    :param b: TensorFlow biases\n",
    "    :return: TensorFlow linear function\n",
    "    \"\"\"\n",
    "    # TODO: Linear Function (xW + b)\n",
    "    return tf.add(tf.matmul(input, w), b)\n",
    "\n",
    "def print_epoch_stats(epoch_i, sess, last_features, last_labels):\n",
    "    \"\"\"\n",
    "    Print cost and validation accuracy of an epoch\n",
    "    \"\"\"\n",
    "    current_cost = sess.run(\n",
    "        cost,\n",
    "        feed_dict={features: last_features, labels: last_labels})\n",
    "    valid_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: valid_features, labels: valid_labels})\n",
    "    print('Epoch: {:<4} - Cost: {:<8.3} Valid Accuracy: {:<5.3}'.format(\n",
    "        epoch_i,\n",
    "        current_cost,\n",
    "        valid_accuracy))\n",
    "    \n",
    "def batches(batch_size, features, labels):\n",
    "    \"\"\"\n",
    "    Create batches of features and labels\n",
    "    :param batch_size: The batch size\n",
    "    :param features: List of features\n",
    "    :param labels: List of labels\n",
    "    :return: Batches of (Features, Labels)\n",
    "    \"\"\"\n",
    "    assert len(features) == len(labels)\n",
    "    # TODO: Implement batching\n",
    "    outout_batches = []\n",
    "    \n",
    "    sample_size = len(features)\n",
    "    for start_i in range(0, sample_size, batch_size):\n",
    "        end_i = start_i + batch_size\n",
    "        batch = [features[start_i:end_i], labels[start_i:end_i]]\n",
    "        outout_batches.append(batch)\n",
    "        \n",
    "    return outout_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0    - Cost: 1.78     Valid Accuracy: 0.727\n",
      "Epoch: 1    - Cost: 1.27     Valid Accuracy: 0.799\n",
      "Epoch: 2    - Cost: 1.06     Valid Accuracy: 0.824\n",
      "Epoch: 3    - Cost: 0.939    Valid Accuracy: 0.841\n",
      "Epoch: 4    - Cost: 0.85     Valid Accuracy: 0.852\n",
      "Epoch: 5    - Cost: 0.783    Valid Accuracy: 0.86 \n",
      "Epoch: 6    - Cost: 0.731    Valid Accuracy: 0.866\n",
      "Epoch: 7    - Cost: 0.69     Valid Accuracy: 0.87 \n",
      "Epoch: 8    - Cost: 0.656    Valid Accuracy: 0.874\n",
      "Epoch: 9    - Cost: 0.629    Valid Accuracy: 0.879\n",
      "Epoch: 10   - Cost: 0.607    Valid Accuracy: 0.881\n",
      "Epoch: 11   - Cost: 0.587    Valid Accuracy: 0.883\n",
      "Epoch: 12   - Cost: 0.57     Valid Accuracy: 0.886\n",
      "Epoch: 13   - Cost: 0.554    Valid Accuracy: 0.889\n",
      "Epoch: 14   - Cost: 0.54     Valid Accuracy: 0.89 \n",
      "Epoch: 15   - Cost: 0.528    Valid Accuracy: 0.892\n",
      "Epoch: 16   - Cost: 0.516    Valid Accuracy: 0.892\n",
      "Epoch: 17   - Cost: 0.505    Valid Accuracy: 0.892\n",
      "Epoch: 18   - Cost: 0.495    Valid Accuracy: 0.894\n",
      "Epoch: 19   - Cost: 0.486    Valid Accuracy: 0.896\n",
      "Epoch: 20   - Cost: 0.477    Valid Accuracy: 0.897\n",
      "Epoch: 21   - Cost: 0.468    Valid Accuracy: 0.897\n",
      "Epoch: 22   - Cost: 0.46     Valid Accuracy: 0.899\n",
      "Epoch: 23   - Cost: 0.453    Valid Accuracy: 0.9  \n",
      "Epoch: 24   - Cost: 0.446    Valid Accuracy: 0.9  \n",
      "Epoch: 25   - Cost: 0.439    Valid Accuracy: 0.901\n",
      "Epoch: 26   - Cost: 0.433    Valid Accuracy: 0.902\n",
      "Epoch: 27   - Cost: 0.427    Valid Accuracy: 0.902\n",
      "Epoch: 28   - Cost: 0.421    Valid Accuracy: 0.903\n",
      "Epoch: 29   - Cost: 0.415    Valid Accuracy: 0.904\n",
      "Epoch: 30   - Cost: 0.41     Valid Accuracy: 0.905\n",
      "Epoch: 31   - Cost: 0.405    Valid Accuracy: 0.906\n",
      "Epoch: 32   - Cost: 0.4      Valid Accuracy: 0.907\n",
      "Epoch: 33   - Cost: 0.395    Valid Accuracy: 0.907\n",
      "Epoch: 34   - Cost: 0.391    Valid Accuracy: 0.908\n",
      "Epoch: 35   - Cost: 0.386    Valid Accuracy: 0.908\n",
      "Epoch: 36   - Cost: 0.382    Valid Accuracy: 0.908\n",
      "Epoch: 37   - Cost: 0.378    Valid Accuracy: 0.908\n",
      "Epoch: 38   - Cost: 0.374    Valid Accuracy: 0.909\n",
      "Epoch: 39   - Cost: 0.371    Valid Accuracy: 0.91 \n",
      "Epoch: 40   - Cost: 0.367    Valid Accuracy: 0.91 \n",
      "Epoch: 41   - Cost: 0.364    Valid Accuracy: 0.911\n",
      "Epoch: 42   - Cost: 0.36     Valid Accuracy: 0.911\n",
      "Epoch: 43   - Cost: 0.357    Valid Accuracy: 0.911\n",
      "Epoch: 44   - Cost: 0.354    Valid Accuracy: 0.911\n",
      "Epoch: 45   - Cost: 0.351    Valid Accuracy: 0.912\n",
      "Epoch: 46   - Cost: 0.348    Valid Accuracy: 0.912\n",
      "Epoch: 47   - Cost: 0.345    Valid Accuracy: 0.912\n",
      "Epoch: 48   - Cost: 0.343    Valid Accuracy: 0.912\n",
      "Epoch: 49   - Cost: 0.34     Valid Accuracy: 0.912\n",
      "Epoch: 50   - Cost: 0.338    Valid Accuracy: 0.912\n",
      "Epoch: 51   - Cost: 0.335    Valid Accuracy: 0.913\n",
      "Epoch: 52   - Cost: 0.333    Valid Accuracy: 0.913\n",
      "Epoch: 53   - Cost: 0.33     Valid Accuracy: 0.912\n",
      "Epoch: 54   - Cost: 0.328    Valid Accuracy: 0.913\n",
      "Epoch: 55   - Cost: 0.326    Valid Accuracy: 0.913\n",
      "Epoch: 56   - Cost: 0.324    Valid Accuracy: 0.913\n",
      "Epoch: 57   - Cost: 0.322    Valid Accuracy: 0.913\n",
      "Epoch: 58   - Cost: 0.32     Valid Accuracy: 0.913\n",
      "Epoch: 59   - Cost: 0.318    Valid Accuracy: 0.913\n",
      "Epoch: 60   - Cost: 0.316    Valid Accuracy: 0.914\n",
      "Epoch: 61   - Cost: 0.314    Valid Accuracy: 0.914\n",
      "Epoch: 62   - Cost: 0.312    Valid Accuracy: 0.914\n",
      "Epoch: 63   - Cost: 0.311    Valid Accuracy: 0.914\n",
      "Epoch: 64   - Cost: 0.309    Valid Accuracy: 0.915\n",
      "Epoch: 65   - Cost: 0.307    Valid Accuracy: 0.915\n",
      "Epoch: 66   - Cost: 0.306    Valid Accuracy: 0.915\n",
      "Epoch: 67   - Cost: 0.304    Valid Accuracy: 0.915\n",
      "Epoch: 68   - Cost: 0.303    Valid Accuracy: 0.915\n",
      "Epoch: 69   - Cost: 0.301    Valid Accuracy: 0.915\n",
      "Epoch: 70   - Cost: 0.3      Valid Accuracy: 0.915\n",
      "Epoch: 71   - Cost: 0.298    Valid Accuracy: 0.916\n",
      "Epoch: 72   - Cost: 0.297    Valid Accuracy: 0.916\n",
      "Epoch: 73   - Cost: 0.296    Valid Accuracy: 0.916\n",
      "Epoch: 74   - Cost: 0.294    Valid Accuracy: 0.916\n",
      "Epoch: 75   - Cost: 0.293    Valid Accuracy: 0.916\n",
      "Epoch: 76   - Cost: 0.292    Valid Accuracy: 0.917\n",
      "Epoch: 77   - Cost: 0.291    Valid Accuracy: 0.917\n",
      "Epoch: 78   - Cost: 0.289    Valid Accuracy: 0.917\n",
      "Epoch: 79   - Cost: 0.288    Valid Accuracy: 0.917\n",
      "Epoch: 80   - Cost: 0.287    Valid Accuracy: 0.917\n",
      "Epoch: 81   - Cost: 0.286    Valid Accuracy: 0.917\n",
      "Epoch: 82   - Cost: 0.285    Valid Accuracy: 0.917\n",
      "Epoch: 83   - Cost: 0.284    Valid Accuracy: 0.917\n",
      "Epoch: 84   - Cost: 0.283    Valid Accuracy: 0.917\n",
      "Epoch: 85   - Cost: 0.282    Valid Accuracy: 0.917\n",
      "Epoch: 86   - Cost: 0.281    Valid Accuracy: 0.918\n",
      "Epoch: 87   - Cost: 0.28     Valid Accuracy: 0.918\n",
      "Epoch: 88   - Cost: 0.279    Valid Accuracy: 0.917\n",
      "Epoch: 89   - Cost: 0.278    Valid Accuracy: 0.917\n",
      "Epoch: 90   - Cost: 0.277    Valid Accuracy: 0.917\n",
      "Epoch: 91   - Cost: 0.276    Valid Accuracy: 0.918\n",
      "Epoch: 92   - Cost: 0.275    Valid Accuracy: 0.917\n",
      "Epoch: 93   - Cost: 0.274    Valid Accuracy: 0.918\n",
      "Epoch: 94   - Cost: 0.274    Valid Accuracy: 0.918\n",
      "Epoch: 95   - Cost: 0.273    Valid Accuracy: 0.918\n",
      "Epoch: 96   - Cost: 0.272    Valid Accuracy: 0.918\n",
      "Epoch: 97   - Cost: 0.271    Valid Accuracy: 0.919\n",
      "Epoch: 98   - Cost: 0.27     Valid Accuracy: 0.919\n",
      "Epoch: 99   - Cost: 0.27     Valid Accuracy: 0.919\n",
      "Epoch: 100  - Cost: 0.269    Valid Accuracy: 0.919\n",
      "Epoch: 101  - Cost: 0.268    Valid Accuracy: 0.919\n",
      "Epoch: 102  - Cost: 0.267    Valid Accuracy: 0.919\n",
      "Epoch: 103  - Cost: 0.267    Valid Accuracy: 0.919\n",
      "Epoch: 104  - Cost: 0.266    Valid Accuracy: 0.919\n",
      "Epoch: 105  - Cost: 0.265    Valid Accuracy: 0.919\n",
      "Epoch: 106  - Cost: 0.264    Valid Accuracy: 0.919\n",
      "Epoch: 107  - Cost: 0.264    Valid Accuracy: 0.919\n",
      "Epoch: 108  - Cost: 0.263    Valid Accuracy: 0.919\n",
      "Epoch: 109  - Cost: 0.262    Valid Accuracy: 0.919\n",
      "Epoch: 110  - Cost: 0.262    Valid Accuracy: 0.919\n",
      "Epoch: 111  - Cost: 0.261    Valid Accuracy: 0.919\n",
      "Epoch: 112  - Cost: 0.261    Valid Accuracy: 0.919\n",
      "Epoch: 113  - Cost: 0.26     Valid Accuracy: 0.919\n",
      "Epoch: 114  - Cost: 0.259    Valid Accuracy: 0.919\n",
      "Epoch: 115  - Cost: 0.259    Valid Accuracy: 0.919\n",
      "Epoch: 116  - Cost: 0.258    Valid Accuracy: 0.919\n",
      "Epoch: 117  - Cost: 0.258    Valid Accuracy: 0.919\n",
      "Epoch: 118  - Cost: 0.257    Valid Accuracy: 0.919\n",
      "Epoch: 119  - Cost: 0.256    Valid Accuracy: 0.919\n",
      "Epoch: 120  - Cost: 0.256    Valid Accuracy: 0.919\n",
      "Epoch: 121  - Cost: 0.255    Valid Accuracy: 0.919\n",
      "Epoch: 122  - Cost: 0.255    Valid Accuracy: 0.919\n",
      "Epoch: 123  - Cost: 0.254    Valid Accuracy: 0.919\n",
      "Epoch: 124  - Cost: 0.254    Valid Accuracy: 0.919\n",
      "Epoch: 125  - Cost: 0.253    Valid Accuracy: 0.919\n",
      "Epoch: 126  - Cost: 0.253    Valid Accuracy: 0.919\n",
      "Epoch: 127  - Cost: 0.252    Valid Accuracy: 0.92 \n",
      "Epoch: 128  - Cost: 0.252    Valid Accuracy: 0.92 \n",
      "Epoch: 129  - Cost: 0.251    Valid Accuracy: 0.92 \n",
      "Epoch: 130  - Cost: 0.251    Valid Accuracy: 0.92 \n",
      "Epoch: 131  - Cost: 0.25     Valid Accuracy: 0.92 \n",
      "Epoch: 132  - Cost: 0.25     Valid Accuracy: 0.92 \n",
      "Epoch: 133  - Cost: 0.249    Valid Accuracy: 0.92 \n",
      "Epoch: 134  - Cost: 0.249    Valid Accuracy: 0.92 \n",
      "Epoch: 135  - Cost: 0.248    Valid Accuracy: 0.92 \n",
      "Epoch: 136  - Cost: 0.248    Valid Accuracy: 0.92 \n",
      "Epoch: 137  - Cost: 0.247    Valid Accuracy: 0.92 \n",
      "Epoch: 138  - Cost: 0.247    Valid Accuracy: 0.92 \n",
      "Epoch: 139  - Cost: 0.246    Valid Accuracy: 0.921\n",
      "Epoch: 140  - Cost: 0.246    Valid Accuracy: 0.921\n",
      "Epoch: 141  - Cost: 0.246    Valid Accuracy: 0.921\n",
      "Epoch: 142  - Cost: 0.245    Valid Accuracy: 0.922\n",
      "Epoch: 143  - Cost: 0.245    Valid Accuracy: 0.922\n",
      "Epoch: 144  - Cost: 0.244    Valid Accuracy: 0.922\n",
      "Epoch: 145  - Cost: 0.244    Valid Accuracy: 0.922\n",
      "Epoch: 146  - Cost: 0.244    Valid Accuracy: 0.922\n",
      "Epoch: 147  - Cost: 0.243    Valid Accuracy: 0.922\n",
      "Epoch: 148  - Cost: 0.243    Valid Accuracy: 0.922\n",
      "Epoch: 149  - Cost: 0.242    Valid Accuracy: 0.922\n",
      "Epoch: 150  - Cost: 0.242    Valid Accuracy: 0.922\n",
      "Epoch: 151  - Cost: 0.242    Valid Accuracy: 0.922\n",
      "Epoch: 152  - Cost: 0.241    Valid Accuracy: 0.921\n",
      "Epoch: 153  - Cost: 0.241    Valid Accuracy: 0.921\n",
      "Epoch: 154  - Cost: 0.24     Valid Accuracy: 0.921\n",
      "Epoch: 155  - Cost: 0.24     Valid Accuracy: 0.921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 156  - Cost: 0.24     Valid Accuracy: 0.921\n",
      "Epoch: 157  - Cost: 0.239    Valid Accuracy: 0.922\n",
      "Epoch: 158  - Cost: 0.239    Valid Accuracy: 0.921\n",
      "Epoch: 159  - Cost: 0.239    Valid Accuracy: 0.921\n",
      "Epoch: 160  - Cost: 0.238    Valid Accuracy: 0.921\n",
      "Epoch: 161  - Cost: 0.238    Valid Accuracy: 0.921\n",
      "Epoch: 162  - Cost: 0.238    Valid Accuracy: 0.922\n",
      "Epoch: 163  - Cost: 0.237    Valid Accuracy: 0.922\n",
      "Epoch: 164  - Cost: 0.237    Valid Accuracy: 0.922\n",
      "Epoch: 165  - Cost: 0.237    Valid Accuracy: 0.921\n",
      "Epoch: 166  - Cost: 0.236    Valid Accuracy: 0.921\n",
      "Epoch: 167  - Cost: 0.236    Valid Accuracy: 0.922\n",
      "Epoch: 168  - Cost: 0.236    Valid Accuracy: 0.922\n",
      "Epoch: 169  - Cost: 0.235    Valid Accuracy: 0.922\n",
      "Epoch: 170  - Cost: 0.235    Valid Accuracy: 0.922\n",
      "Epoch: 171  - Cost: 0.235    Valid Accuracy: 0.922\n",
      "Epoch: 172  - Cost: 0.234    Valid Accuracy: 0.922\n",
      "Epoch: 173  - Cost: 0.234    Valid Accuracy: 0.922\n",
      "Epoch: 174  - Cost: 0.234    Valid Accuracy: 0.922\n",
      "Epoch: 175  - Cost: 0.233    Valid Accuracy: 0.922\n",
      "Epoch: 176  - Cost: 0.233    Valid Accuracy: 0.922\n",
      "Epoch: 177  - Cost: 0.233    Valid Accuracy: 0.922\n",
      "Epoch: 178  - Cost: 0.232    Valid Accuracy: 0.922\n",
      "Epoch: 179  - Cost: 0.232    Valid Accuracy: 0.922\n",
      "Epoch: 180  - Cost: 0.232    Valid Accuracy: 0.922\n",
      "Epoch: 181  - Cost: 0.232    Valid Accuracy: 0.923\n",
      "Epoch: 182  - Cost: 0.231    Valid Accuracy: 0.923\n",
      "Epoch: 183  - Cost: 0.231    Valid Accuracy: 0.923\n",
      "Epoch: 184  - Cost: 0.231    Valid Accuracy: 0.923\n",
      "Epoch: 185  - Cost: 0.23     Valid Accuracy: 0.923\n",
      "Epoch: 186  - Cost: 0.23     Valid Accuracy: 0.923\n",
      "Epoch: 187  - Cost: 0.23     Valid Accuracy: 0.922\n",
      "Epoch: 188  - Cost: 0.23     Valid Accuracy: 0.922\n",
      "Epoch: 189  - Cost: 0.229    Valid Accuracy: 0.923\n",
      "Epoch: 190  - Cost: 0.229    Valid Accuracy: 0.923\n",
      "Epoch: 191  - Cost: 0.229    Valid Accuracy: 0.923\n",
      "Epoch: 192  - Cost: 0.229    Valid Accuracy: 0.923\n",
      "Epoch: 193  - Cost: 0.228    Valid Accuracy: 0.923\n",
      "Epoch: 194  - Cost: 0.228    Valid Accuracy: 0.922\n",
      "Epoch: 195  - Cost: 0.228    Valid Accuracy: 0.923\n",
      "Epoch: 196  - Cost: 0.228    Valid Accuracy: 0.923\n",
      "Epoch: 197  - Cost: 0.227    Valid Accuracy: 0.923\n",
      "Epoch: 198  - Cost: 0.227    Valid Accuracy: 0.923\n",
      "Epoch: 199  - Cost: 0.227    Valid Accuracy: 0.923\n",
      "Epoch: 200  - Cost: 0.227    Valid Accuracy: 0.923\n",
      "Epoch: 201  - Cost: 0.226    Valid Accuracy: 0.923\n",
      "Epoch: 202  - Cost: 0.226    Valid Accuracy: 0.923\n",
      "Epoch: 203  - Cost: 0.226    Valid Accuracy: 0.923\n",
      "Epoch: 204  - Cost: 0.226    Valid Accuracy: 0.923\n",
      "Epoch: 205  - Cost: 0.225    Valid Accuracy: 0.923\n",
      "Epoch: 206  - Cost: 0.225    Valid Accuracy: 0.923\n",
      "Epoch: 207  - Cost: 0.225    Valid Accuracy: 0.923\n",
      "Epoch: 208  - Cost: 0.225    Valid Accuracy: 0.923\n",
      "Epoch: 209  - Cost: 0.224    Valid Accuracy: 0.923\n",
      "Epoch: 210  - Cost: 0.224    Valid Accuracy: 0.923\n",
      "Epoch: 211  - Cost: 0.224    Valid Accuracy: 0.923\n",
      "Epoch: 212  - Cost: 0.224    Valid Accuracy: 0.923\n",
      "Epoch: 213  - Cost: 0.223    Valid Accuracy: 0.923\n",
      "Epoch: 214  - Cost: 0.223    Valid Accuracy: 0.923\n",
      "Epoch: 215  - Cost: 0.223    Valid Accuracy: 0.923\n",
      "Epoch: 216  - Cost: 0.223    Valid Accuracy: 0.923\n",
      "Epoch: 217  - Cost: 0.223    Valid Accuracy: 0.923\n",
      "Epoch: 218  - Cost: 0.222    Valid Accuracy: 0.923\n",
      "Epoch: 219  - Cost: 0.222    Valid Accuracy: 0.923\n",
      "Epoch: 220  - Cost: 0.222    Valid Accuracy: 0.923\n",
      "Epoch: 221  - Cost: 0.222    Valid Accuracy: 0.924\n",
      "Epoch: 222  - Cost: 0.221    Valid Accuracy: 0.924\n",
      "Epoch: 223  - Cost: 0.221    Valid Accuracy: 0.924\n",
      "Epoch: 224  - Cost: 0.221    Valid Accuracy: 0.924\n",
      "Epoch: 225  - Cost: 0.221    Valid Accuracy: 0.924\n",
      "Epoch: 226  - Cost: 0.221    Valid Accuracy: 0.924\n",
      "Epoch: 227  - Cost: 0.22     Valid Accuracy: 0.924\n",
      "Epoch: 228  - Cost: 0.22     Valid Accuracy: 0.924\n",
      "Epoch: 229  - Cost: 0.22     Valid Accuracy: 0.924\n",
      "Epoch: 230  - Cost: 0.22     Valid Accuracy: 0.924\n",
      "Epoch: 231  - Cost: 0.22     Valid Accuracy: 0.924\n",
      "Epoch: 232  - Cost: 0.219    Valid Accuracy: 0.924\n",
      "Epoch: 233  - Cost: 0.219    Valid Accuracy: 0.924\n",
      "Epoch: 234  - Cost: 0.219    Valid Accuracy: 0.924\n",
      "Epoch: 235  - Cost: 0.219    Valid Accuracy: 0.924\n",
      "Epoch: 236  - Cost: 0.219    Valid Accuracy: 0.924\n",
      "Epoch: 237  - Cost: 0.218    Valid Accuracy: 0.924\n",
      "Epoch: 238  - Cost: 0.218    Valid Accuracy: 0.924\n",
      "Epoch: 239  - Cost: 0.218    Valid Accuracy: 0.924\n",
      "Epoch: 240  - Cost: 0.218    Valid Accuracy: 0.924\n",
      "Epoch: 241  - Cost: 0.218    Valid Accuracy: 0.924\n",
      "Epoch: 242  - Cost: 0.218    Valid Accuracy: 0.924\n",
      "Epoch: 243  - Cost: 0.217    Valid Accuracy: 0.924\n",
      "Epoch: 244  - Cost: 0.217    Valid Accuracy: 0.924\n",
      "Epoch: 245  - Cost: 0.217    Valid Accuracy: 0.924\n",
      "Epoch: 246  - Cost: 0.217    Valid Accuracy: 0.924\n",
      "Epoch: 247  - Cost: 0.217    Valid Accuracy: 0.924\n",
      "Epoch: 248  - Cost: 0.216    Valid Accuracy: 0.924\n",
      "Epoch: 249  - Cost: 0.216    Valid Accuracy: 0.924\n",
      "Epoch: 250  - Cost: 0.216    Valid Accuracy: 0.924\n",
      "Epoch: 251  - Cost: 0.216    Valid Accuracy: 0.924\n",
      "Epoch: 252  - Cost: 0.216    Valid Accuracy: 0.923\n",
      "Epoch: 253  - Cost: 0.216    Valid Accuracy: 0.923\n",
      "Epoch: 254  - Cost: 0.215    Valid Accuracy: 0.923\n",
      "Epoch: 255  - Cost: 0.215    Valid Accuracy: 0.924\n",
      "Epoch: 256  - Cost: 0.215    Valid Accuracy: 0.924\n",
      "Epoch: 257  - Cost: 0.215    Valid Accuracy: 0.924\n",
      "Epoch: 258  - Cost: 0.215    Valid Accuracy: 0.924\n",
      "Epoch: 259  - Cost: 0.215    Valid Accuracy: 0.923\n",
      "Epoch: 260  - Cost: 0.214    Valid Accuracy: 0.923\n",
      "Epoch: 261  - Cost: 0.214    Valid Accuracy: 0.923\n",
      "Epoch: 262  - Cost: 0.214    Valid Accuracy: 0.923\n",
      "Epoch: 263  - Cost: 0.214    Valid Accuracy: 0.923\n",
      "Epoch: 264  - Cost: 0.214    Valid Accuracy: 0.923\n",
      "Epoch: 265  - Cost: 0.214    Valid Accuracy: 0.923\n",
      "Epoch: 266  - Cost: 0.213    Valid Accuracy: 0.923\n",
      "Epoch: 267  - Cost: 0.213    Valid Accuracy: 0.923\n",
      "Epoch: 268  - Cost: 0.213    Valid Accuracy: 0.923\n",
      "Epoch: 269  - Cost: 0.213    Valid Accuracy: 0.923\n",
      "Epoch: 270  - Cost: 0.213    Valid Accuracy: 0.923\n",
      "Epoch: 271  - Cost: 0.213    Valid Accuracy: 0.924\n",
      "Epoch: 272  - Cost: 0.213    Valid Accuracy: 0.924\n",
      "Epoch: 273  - Cost: 0.212    Valid Accuracy: 0.924\n",
      "Epoch: 274  - Cost: 0.212    Valid Accuracy: 0.924\n",
      "Epoch: 275  - Cost: 0.212    Valid Accuracy: 0.924\n",
      "Epoch: 276  - Cost: 0.212    Valid Accuracy: 0.924\n",
      "Epoch: 277  - Cost: 0.212    Valid Accuracy: 0.924\n",
      "Epoch: 278  - Cost: 0.212    Valid Accuracy: 0.924\n",
      "Epoch: 279  - Cost: 0.211    Valid Accuracy: 0.924\n",
      "Epoch: 280  - Cost: 0.211    Valid Accuracy: 0.924\n",
      "Epoch: 281  - Cost: 0.211    Valid Accuracy: 0.924\n",
      "Epoch: 282  - Cost: 0.211    Valid Accuracy: 0.925\n",
      "Epoch: 283  - Cost: 0.211    Valid Accuracy: 0.924\n",
      "Epoch: 284  - Cost: 0.211    Valid Accuracy: 0.924\n",
      "Epoch: 285  - Cost: 0.211    Valid Accuracy: 0.924\n",
      "Epoch: 286  - Cost: 0.21     Valid Accuracy: 0.924\n",
      "Epoch: 287  - Cost: 0.21     Valid Accuracy: 0.924\n",
      "Epoch: 288  - Cost: 0.21     Valid Accuracy: 0.924\n",
      "Epoch: 289  - Cost: 0.21     Valid Accuracy: 0.924\n",
      "Epoch: 290  - Cost: 0.21     Valid Accuracy: 0.924\n",
      "Epoch: 291  - Cost: 0.21     Valid Accuracy: 0.924\n",
      "Epoch: 292  - Cost: 0.21     Valid Accuracy: 0.924\n",
      "Epoch: 293  - Cost: 0.209    Valid Accuracy: 0.924\n",
      "Epoch: 294  - Cost: 0.209    Valid Accuracy: 0.924\n",
      "Epoch: 295  - Cost: 0.209    Valid Accuracy: 0.924\n",
      "Epoch: 296  - Cost: 0.209    Valid Accuracy: 0.924\n",
      "Epoch: 297  - Cost: 0.209    Valid Accuracy: 0.924\n",
      "Epoch: 298  - Cost: 0.209    Valid Accuracy: 0.924\n",
      "Epoch: 299  - Cost: 0.209    Valid Accuracy: 0.924\n",
      "Epoch: 300  - Cost: 0.209    Valid Accuracy: 0.924\n",
      "Epoch: 301  - Cost: 0.208    Valid Accuracy: 0.924\n",
      "Epoch: 302  - Cost: 0.208    Valid Accuracy: 0.924\n",
      "Epoch: 303  - Cost: 0.208    Valid Accuracy: 0.924\n",
      "Epoch: 304  - Cost: 0.208    Valid Accuracy: 0.924\n",
      "Epoch: 305  - Cost: 0.208    Valid Accuracy: 0.924\n",
      "Epoch: 306  - Cost: 0.208    Valid Accuracy: 0.924\n",
      "Epoch: 307  - Cost: 0.208    Valid Accuracy: 0.924\n",
      "Epoch: 308  - Cost: 0.208    Valid Accuracy: 0.924\n",
      "Epoch: 309  - Cost: 0.207    Valid Accuracy: 0.924\n",
      "Epoch: 310  - Cost: 0.207    Valid Accuracy: 0.924\n",
      "Epoch: 311  - Cost: 0.207    Valid Accuracy: 0.924\n",
      "Epoch: 312  - Cost: 0.207    Valid Accuracy: 0.924\n",
      "Epoch: 313  - Cost: 0.207    Valid Accuracy: 0.924\n",
      "Epoch: 314  - Cost: 0.207    Valid Accuracy: 0.924\n",
      "Epoch: 315  - Cost: 0.207    Valid Accuracy: 0.924\n",
      "Epoch: 316  - Cost: 0.207    Valid Accuracy: 0.924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 317  - Cost: 0.206    Valid Accuracy: 0.924\n",
      "Epoch: 318  - Cost: 0.206    Valid Accuracy: 0.924\n",
      "Epoch: 319  - Cost: 0.206    Valid Accuracy: 0.924\n",
      "Epoch: 320  - Cost: 0.206    Valid Accuracy: 0.924\n",
      "Epoch: 321  - Cost: 0.206    Valid Accuracy: 0.924\n",
      "Epoch: 322  - Cost: 0.206    Valid Accuracy: 0.924\n",
      "Epoch: 323  - Cost: 0.206    Valid Accuracy: 0.924\n",
      "Epoch: 324  - Cost: 0.206    Valid Accuracy: 0.924\n",
      "Epoch: 325  - Cost: 0.205    Valid Accuracy: 0.924\n",
      "Epoch: 326  - Cost: 0.205    Valid Accuracy: 0.924\n",
      "Epoch: 327  - Cost: 0.205    Valid Accuracy: 0.924\n",
      "Epoch: 328  - Cost: 0.205    Valid Accuracy: 0.924\n",
      "Epoch: 329  - Cost: 0.205    Valid Accuracy: 0.924\n",
      "Epoch: 330  - Cost: 0.205    Valid Accuracy: 0.924\n",
      "Epoch: 331  - Cost: 0.205    Valid Accuracy: 0.924\n",
      "Epoch: 332  - Cost: 0.205    Valid Accuracy: 0.924\n",
      "Epoch: 333  - Cost: 0.205    Valid Accuracy: 0.924\n",
      "Epoch: 334  - Cost: 0.204    Valid Accuracy: 0.924\n",
      "Epoch: 335  - Cost: 0.204    Valid Accuracy: 0.924\n",
      "Epoch: 336  - Cost: 0.204    Valid Accuracy: 0.925\n",
      "Epoch: 337  - Cost: 0.204    Valid Accuracy: 0.925\n",
      "Epoch: 338  - Cost: 0.204    Valid Accuracy: 0.925\n",
      "Epoch: 339  - Cost: 0.204    Valid Accuracy: 0.925\n",
      "Epoch: 340  - Cost: 0.204    Valid Accuracy: 0.925\n",
      "Epoch: 341  - Cost: 0.204    Valid Accuracy: 0.925\n",
      "Epoch: 342  - Cost: 0.204    Valid Accuracy: 0.925\n",
      "Epoch: 343  - Cost: 0.203    Valid Accuracy: 0.925\n",
      "Epoch: 344  - Cost: 0.203    Valid Accuracy: 0.925\n",
      "Epoch: 345  - Cost: 0.203    Valid Accuracy: 0.925\n",
      "Epoch: 346  - Cost: 0.203    Valid Accuracy: 0.925\n",
      "Epoch: 347  - Cost: 0.203    Valid Accuracy: 0.925\n",
      "Epoch: 348  - Cost: 0.203    Valid Accuracy: 0.925\n",
      "Epoch: 349  - Cost: 0.203    Valid Accuracy: 0.925\n",
      "Epoch: 350  - Cost: 0.203    Valid Accuracy: 0.925\n",
      "Epoch: 351  - Cost: 0.203    Valid Accuracy: 0.925\n",
      "Epoch: 352  - Cost: 0.203    Valid Accuracy: 0.925\n",
      "Epoch: 353  - Cost: 0.202    Valid Accuracy: 0.925\n",
      "Epoch: 354  - Cost: 0.202    Valid Accuracy: 0.925\n",
      "Epoch: 355  - Cost: 0.202    Valid Accuracy: 0.925\n",
      "Epoch: 356  - Cost: 0.202    Valid Accuracy: 0.924\n",
      "Epoch: 357  - Cost: 0.202    Valid Accuracy: 0.924\n",
      "Epoch: 358  - Cost: 0.202    Valid Accuracy: 0.924\n",
      "Epoch: 359  - Cost: 0.202    Valid Accuracy: 0.924\n",
      "Epoch: 360  - Cost: 0.202    Valid Accuracy: 0.924\n",
      "Epoch: 361  - Cost: 0.202    Valid Accuracy: 0.924\n",
      "Epoch: 362  - Cost: 0.202    Valid Accuracy: 0.924\n",
      "Epoch: 363  - Cost: 0.201    Valid Accuracy: 0.924\n",
      "Epoch: 364  - Cost: 0.201    Valid Accuracy: 0.924\n",
      "Epoch: 365  - Cost: 0.201    Valid Accuracy: 0.924\n",
      "Epoch: 366  - Cost: 0.201    Valid Accuracy: 0.924\n",
      "Epoch: 367  - Cost: 0.201    Valid Accuracy: 0.924\n",
      "Epoch: 368  - Cost: 0.201    Valid Accuracy: 0.924\n",
      "Epoch: 369  - Cost: 0.201    Valid Accuracy: 0.924\n",
      "Epoch: 370  - Cost: 0.201    Valid Accuracy: 0.924\n",
      "Epoch: 371  - Cost: 0.201    Valid Accuracy: 0.924\n",
      "Epoch: 372  - Cost: 0.201    Valid Accuracy: 0.924\n",
      "Epoch: 373  - Cost: 0.201    Valid Accuracy: 0.924\n",
      "Epoch: 374  - Cost: 0.2      Valid Accuracy: 0.924\n",
      "Epoch: 375  - Cost: 0.2      Valid Accuracy: 0.924\n",
      "Epoch: 376  - Cost: 0.2      Valid Accuracy: 0.924\n",
      "Epoch: 377  - Cost: 0.2      Valid Accuracy: 0.924\n",
      "Epoch: 378  - Cost: 0.2      Valid Accuracy: 0.924\n",
      "Epoch: 379  - Cost: 0.2      Valid Accuracy: 0.924\n",
      "Epoch: 380  - Cost: 0.2      Valid Accuracy: 0.924\n",
      "Epoch: 381  - Cost: 0.2      Valid Accuracy: 0.925\n",
      "Epoch: 382  - Cost: 0.2      Valid Accuracy: 0.925\n",
      "Epoch: 383  - Cost: 0.2      Valid Accuracy: 0.925\n",
      "Epoch: 384  - Cost: 0.2      Valid Accuracy: 0.925\n",
      "Epoch: 385  - Cost: 0.199    Valid Accuracy: 0.925\n",
      "Epoch: 386  - Cost: 0.199    Valid Accuracy: 0.925\n",
      "Epoch: 387  - Cost: 0.199    Valid Accuracy: 0.925\n",
      "Epoch: 388  - Cost: 0.199    Valid Accuracy: 0.925\n",
      "Epoch: 389  - Cost: 0.199    Valid Accuracy: 0.925\n",
      "Epoch: 390  - Cost: 0.199    Valid Accuracy: 0.925\n",
      "Epoch: 391  - Cost: 0.199    Valid Accuracy: 0.925\n",
      "Epoch: 392  - Cost: 0.199    Valid Accuracy: 0.925\n",
      "Epoch: 393  - Cost: 0.199    Valid Accuracy: 0.925\n",
      "Epoch: 394  - Cost: 0.199    Valid Accuracy: 0.925\n",
      "Epoch: 395  - Cost: 0.199    Valid Accuracy: 0.925\n",
      "Epoch: 396  - Cost: 0.198    Valid Accuracy: 0.925\n",
      "Epoch: 397  - Cost: 0.198    Valid Accuracy: 0.925\n",
      "Epoch: 398  - Cost: 0.198    Valid Accuracy: 0.925\n",
      "Epoch: 399  - Cost: 0.198    Valid Accuracy: 0.925\n",
      "Epoch: 400  - Cost: 0.198    Valid Accuracy: 0.925\n",
      "Epoch: 401  - Cost: 0.198    Valid Accuracy: 0.925\n",
      "Epoch: 402  - Cost: 0.198    Valid Accuracy: 0.925\n",
      "Epoch: 403  - Cost: 0.198    Valid Accuracy: 0.925\n",
      "Epoch: 404  - Cost: 0.198    Valid Accuracy: 0.925\n",
      "Epoch: 405  - Cost: 0.198    Valid Accuracy: 0.925\n",
      "Epoch: 406  - Cost: 0.198    Valid Accuracy: 0.925\n",
      "Epoch: 407  - Cost: 0.198    Valid Accuracy: 0.925\n",
      "Epoch: 408  - Cost: 0.197    Valid Accuracy: 0.925\n",
      "Epoch: 409  - Cost: 0.197    Valid Accuracy: 0.925\n",
      "Epoch: 410  - Cost: 0.197    Valid Accuracy: 0.925\n",
      "Epoch: 411  - Cost: 0.197    Valid Accuracy: 0.925\n",
      "Epoch: 412  - Cost: 0.197    Valid Accuracy: 0.925\n",
      "Epoch: 413  - Cost: 0.197    Valid Accuracy: 0.925\n",
      "Epoch: 414  - Cost: 0.197    Valid Accuracy: 0.925\n",
      "Epoch: 415  - Cost: 0.197    Valid Accuracy: 0.925\n",
      "Epoch: 416  - Cost: 0.197    Valid Accuracy: 0.925\n",
      "Epoch: 417  - Cost: 0.197    Valid Accuracy: 0.925\n",
      "Epoch: 418  - Cost: 0.197    Valid Accuracy: 0.925\n",
      "Epoch: 419  - Cost: 0.197    Valid Accuracy: 0.925\n",
      "Epoch: 420  - Cost: 0.197    Valid Accuracy: 0.925\n",
      "Epoch: 421  - Cost: 0.196    Valid Accuracy: 0.925\n",
      "Epoch: 422  - Cost: 0.196    Valid Accuracy: 0.925\n",
      "Epoch: 423  - Cost: 0.196    Valid Accuracy: 0.925\n",
      "Epoch: 424  - Cost: 0.196    Valid Accuracy: 0.925\n",
      "Epoch: 425  - Cost: 0.196    Valid Accuracy: 0.925\n",
      "Epoch: 426  - Cost: 0.196    Valid Accuracy: 0.925\n",
      "Epoch: 427  - Cost: 0.196    Valid Accuracy: 0.926\n",
      "Epoch: 428  - Cost: 0.196    Valid Accuracy: 0.926\n",
      "Epoch: 429  - Cost: 0.196    Valid Accuracy: 0.926\n",
      "Epoch: 430  - Cost: 0.196    Valid Accuracy: 0.926\n",
      "Epoch: 431  - Cost: 0.196    Valid Accuracy: 0.926\n",
      "Epoch: 432  - Cost: 0.196    Valid Accuracy: 0.926\n",
      "Epoch: 433  - Cost: 0.196    Valid Accuracy: 0.926\n",
      "Epoch: 434  - Cost: 0.195    Valid Accuracy: 0.926\n",
      "Epoch: 435  - Cost: 0.195    Valid Accuracy: 0.926\n",
      "Epoch: 436  - Cost: 0.195    Valid Accuracy: 0.926\n",
      "Epoch: 437  - Cost: 0.195    Valid Accuracy: 0.926\n",
      "Epoch: 438  - Cost: 0.195    Valid Accuracy: 0.926\n",
      "Epoch: 439  - Cost: 0.195    Valid Accuracy: 0.926\n",
      "Epoch: 440  - Cost: 0.195    Valid Accuracy: 0.926\n",
      "Epoch: 441  - Cost: 0.195    Valid Accuracy: 0.926\n",
      "Epoch: 442  - Cost: 0.195    Valid Accuracy: 0.926\n",
      "Epoch: 443  - Cost: 0.195    Valid Accuracy: 0.926\n",
      "Epoch: 444  - Cost: 0.195    Valid Accuracy: 0.926\n",
      "Epoch: 445  - Cost: 0.195    Valid Accuracy: 0.926\n",
      "Epoch: 446  - Cost: 0.195    Valid Accuracy: 0.926\n",
      "Epoch: 447  - Cost: 0.195    Valid Accuracy: 0.926\n",
      "Epoch: 448  - Cost: 0.194    Valid Accuracy: 0.926\n",
      "Epoch: 449  - Cost: 0.194    Valid Accuracy: 0.926\n",
      "Epoch: 450  - Cost: 0.194    Valid Accuracy: 0.926\n",
      "Epoch: 451  - Cost: 0.194    Valid Accuracy: 0.926\n",
      "Epoch: 452  - Cost: 0.194    Valid Accuracy: 0.926\n",
      "Epoch: 453  - Cost: 0.194    Valid Accuracy: 0.926\n",
      "Epoch: 454  - Cost: 0.194    Valid Accuracy: 0.926\n",
      "Epoch: 455  - Cost: 0.194    Valid Accuracy: 0.926\n",
      "Epoch: 456  - Cost: 0.194    Valid Accuracy: 0.926\n",
      "Epoch: 457  - Cost: 0.194    Valid Accuracy: 0.926\n",
      "Epoch: 458  - Cost: 0.194    Valid Accuracy: 0.926\n",
      "Epoch: 459  - Cost: 0.194    Valid Accuracy: 0.926\n",
      "Epoch: 460  - Cost: 0.194    Valid Accuracy: 0.926\n",
      "Epoch: 461  - Cost: 0.194    Valid Accuracy: 0.926\n",
      "Epoch: 462  - Cost: 0.194    Valid Accuracy: 0.926\n",
      "Epoch: 463  - Cost: 0.193    Valid Accuracy: 0.926\n",
      "Epoch: 464  - Cost: 0.193    Valid Accuracy: 0.926\n",
      "Epoch: 465  - Cost: 0.193    Valid Accuracy: 0.926\n",
      "Epoch: 466  - Cost: 0.193    Valid Accuracy: 0.926\n",
      "Epoch: 467  - Cost: 0.193    Valid Accuracy: 0.926\n",
      "Epoch: 468  - Cost: 0.193    Valid Accuracy: 0.926\n",
      "Epoch: 469  - Cost: 0.193    Valid Accuracy: 0.926\n",
      "Epoch: 470  - Cost: 0.193    Valid Accuracy: 0.926\n",
      "Epoch: 471  - Cost: 0.193    Valid Accuracy: 0.926\n",
      "Epoch: 472  - Cost: 0.193    Valid Accuracy: 0.926\n",
      "Epoch: 473  - Cost: 0.193    Valid Accuracy: 0.926\n",
      "Epoch: 474  - Cost: 0.193    Valid Accuracy: 0.926\n",
      "Epoch: 475  - Cost: 0.193    Valid Accuracy: 0.926\n",
      "Epoch: 476  - Cost: 0.193    Valid Accuracy: 0.926\n",
      "Epoch: 477  - Cost: 0.193    Valid Accuracy: 0.926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 478  - Cost: 0.193    Valid Accuracy: 0.926\n",
      "Epoch: 479  - Cost: 0.192    Valid Accuracy: 0.926\n",
      "Epoch: 480  - Cost: 0.192    Valid Accuracy: 0.926\n",
      "Epoch: 481  - Cost: 0.192    Valid Accuracy: 0.926\n",
      "Epoch: 482  - Cost: 0.192    Valid Accuracy: 0.926\n",
      "Epoch: 483  - Cost: 0.192    Valid Accuracy: 0.926\n",
      "Epoch: 484  - Cost: 0.192    Valid Accuracy: 0.926\n",
      "Epoch: 485  - Cost: 0.192    Valid Accuracy: 0.926\n",
      "Epoch: 486  - Cost: 0.192    Valid Accuracy: 0.926\n",
      "Epoch: 487  - Cost: 0.192    Valid Accuracy: 0.926\n",
      "Epoch: 488  - Cost: 0.192    Valid Accuracy: 0.926\n",
      "Epoch: 489  - Cost: 0.192    Valid Accuracy: 0.926\n",
      "Epoch: 490  - Cost: 0.192    Valid Accuracy: 0.926\n",
      "Epoch: 491  - Cost: 0.192    Valid Accuracy: 0.926\n",
      "Epoch: 492  - Cost: 0.192    Valid Accuracy: 0.926\n",
      "Epoch: 493  - Cost: 0.192    Valid Accuracy: 0.926\n",
      "Epoch: 494  - Cost: 0.192    Valid Accuracy: 0.926\n",
      "Epoch: 495  - Cost: 0.191    Valid Accuracy: 0.926\n",
      "Epoch: 496  - Cost: 0.191    Valid Accuracy: 0.926\n",
      "Epoch: 497  - Cost: 0.191    Valid Accuracy: 0.926\n",
      "Epoch: 498  - Cost: 0.191    Valid Accuracy: 0.926\n",
      "Epoch: 499  - Cost: 0.191    Valid Accuracy: 0.926\n",
      "Test Accuracy: 0.92330002784729\n"
     ]
    }
   ],
   "source": [
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('/tmp/tensorflow/mnist/input_data', one_hot=True)\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "valid_features = mnist.validation.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "valid_labels = mnist.validation.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "# Logits - xW + b\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "# Define loss and optimizer\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 500\n",
    "learn_rate = 0.09\n",
    "\n",
    "train_batches = batches(batch_size, train_features, train_labels)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch_i in range(epochs):\n",
    "\n",
    "        # Loop over all batches\n",
    "        for batch_features, batch_labels in train_batches:\n",
    "            train_feed_dict = {\n",
    "                features: batch_features,\n",
    "                labels: batch_labels,\n",
    "                learning_rate: learn_rate}\n",
    "            sess.run(optimizer, feed_dict=train_feed_dict)\n",
    "\n",
    "        # Print cost and validation accuracy of an epoch\n",
    "        print_epoch_stats(epoch_i, sess, batch_features, batch_labels)\n",
    "\n",
    "    # Calculate accuracy for test dataset\n",
    "    test_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: test_features, labels: test_labels})\n",
    "\n",
    "print('Test Accuracy: {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### SOMETHING TO TRY TO GET HANG OF IT ... IF ABOVE IS TOO COMPLEX .. \n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "# ## Above Block Helper Functions can be modlarized to separate file .. if so\n",
    "# ## below import\n",
    "# #from quiz import weights, biases, linear \n",
    "\n",
    "# def mnist_features_labels(n_labels):\n",
    "#     \"\"\"\n",
    "#     Gets the first <n> labels from the MNIST dataset\n",
    "#     :param n_labels: Number of labels to use\n",
    "#     :return: Tuple of feature list and label list\n",
    "#     \"\"\"\n",
    "#     mnist_features = []\n",
    "#     mnist_labels = []\n",
    "\n",
    "#     #mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
    "#     mnist = input_data.read_data_sets('/tmp/tensorflow/mnist/input_data', one_hot=True)\n",
    "\n",
    "#     # In order to make quizzes run faster, we're only looking at 10000 images\n",
    "#     for mnist_feature, mnist_label in zip(*mnist.train.next_batch(10000)):\n",
    "        \n",
    "#         #print (\"Before Checking\", mnist_feature, mnist_label)\n",
    "        \n",
    "#         # Add features and labels if it's for the first <n>th labels\n",
    "#         # In this eample since n_labels is 3, ie. features & labels for only numbers 0, 1, 2 will be retrieved\n",
    "#         # from dataset remember each feature is 28*28 image and is flattened to 784 (1*784 Matrix)\n",
    "#         if mnist_label[:n_labels].any():\n",
    "#             #If Label is any of top 3, add feature for current index record\n",
    "#             mnist_features.append(mnist_feature)\n",
    "#             #If Label is any of top 3, add label values for just top 3 indexes, we do not need rest as we are not\n",
    "#             # looking to classify anything after number 3..\n",
    "#             mnist_labels.append(mnist_label[:n_labels])\n",
    "\n",
    "#     return mnist_features, mnist_labels\n",
    "\n",
    "\n",
    "# # Number of features (28*28 image is 784 features)\n",
    "# n_features = 784\n",
    "# # Number of labels\n",
    "# n_labels = 3\n",
    "\n",
    "# # Features and Labels\n",
    "# features = tf.placeholder(tf.float32)\n",
    "# labels = tf.placeholder(tf.float32)\n",
    "\n",
    "# # Weights and Biases\n",
    "# w = weights(n_features, n_labels)\n",
    "# b = biases(n_labels)\n",
    "\n",
    "# # Linear Function xW + b\n",
    "# logits = linear(features, w, b)\n",
    "\n",
    "\n",
    "# # Training data\n",
    "# train_features, train_labels = mnist_features_labels(n_labels)\n",
    "\n",
    "# with tf.Session() as session:\n",
    "#     session.run(tf.global_variables_initializer())\n",
    "\n",
    "#     # Softmax (Thus far we studied Sigmoid, it will be good to google understanding differences between them)\n",
    "#     prediction = tf.nn.softmax(logits)\n",
    "\n",
    "#     # Cross entropy\n",
    "#     # This quantifies how far off the predictions were.\n",
    "#     # You'll learn more about this in future lessons.\n",
    "#     cross_entropy = -tf.reduce_sum(labels * tf.log(prediction), reduction_indices=1)\n",
    "\n",
    "#     # Training loss\n",
    "#     # You'll learn more about this in future lessons.\n",
    "#     loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "#     # Rate at which the weights are changed\n",
    "#     # You'll learn more about this in future lessons.\n",
    "#     learning_rate = 0.08\n",
    "\n",
    "#     # Gradient Descent\n",
    "#     # This is the method used to train the model\n",
    "#     # You'll learn more about this in future lessons.\n",
    "#     optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "#     # Run optimizer and get loss\n",
    "#     _, l = session.run(\n",
    "#         [optimizer, loss],\n",
    "#         feed_dict={features: train_features, labels: train_labels})\n",
    "\n",
    "# # Print loss\n",
    "# print('Loss: {}'.format(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
